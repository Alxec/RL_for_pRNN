defaults:
    - rl/ppo # Choose algorithm subconfig
    - _self_  
    - override hydra/hydra_logging: disabled  
    - override hydra/job_logging: disabled

hydra:  
  output_subdir: null
  sweep:
    dir: .
    subdir: .
  run:  
    dir: .

exp:
    env_name: MiniGrid-Lava-Corners-neg05-v2 # Name of the environment to train on [MiniGrid-DonutLava-Long-neg05-v2]
    opt_return: 0.89 # Optimal return for the environment
    # wrapper: 
    #   _target_: minigrid.wrappers.RGBImgPartialObsWrapper_HD # Environment wrapper as python class
    input_type: 'FO' # ['Visual_FO', 'Visual_PO', 'PC', 'CANN', 'PC+PO', 'pRNN', 'pRNN+PO','CANN+PO', 'CANN_norecurrence']
    exp_name: test
    seed: 2 # Random seed
    spatial: False # Whether AC uses spatial input
    pRNN: False # Whether to use pRNN for spatial input
    PC: False # Whether to use fake place cells for spatial input
    CANN: False # Whether to use CANN for spatial input
    recurrence: 1 # Whether to use RNN for memory (1 = don't use, other number = number of recurrent steps)
    with_obs: False # Whether to include visual input (in case of recurrency the one that would bypass RNN)
    with_HD: False
    rgb: False # Is visual input RGB?
    intrinsic: False # Use intrinsic rewards?
    theta: False
    shared_weights: False
    single_theta: False

hardware:
    use_gpu: True
    which_gpu: 0 # GPU device code, not used now


rl:
    steps: 5e5
    discount: 0.98
    lr: 0.0007
    gae_lambda: 0.95 # Lambda coefficient in GAE formula (1 means no gae)
    entropy_coef: 0.01
    value_loss_coef: 0.5
    max_grad_norm: 0.5
    optim_eps: 1e-8 # Adam and RMSprop optimizer epsilon
    optim_alpha: 0.99 # RMSprop optimizer alpha
    pc_sd: 4
    pc_norm: True
    cann_input_width: 100
    k_int: 1 # Coefficient for intrinsic reward
    eval_type: rewards
    value_type: single

predNet:
    path: example_nets_donut_long_theta/thcycRNN_5win_full-theta-s8 #example_nets_donut_long_prev_action/thRNN_5win_prevAct-PrevAct-s8
    folder: '/home/mila/a/aleksei.efremov/pRNN-RL/LabNotebook/20250912_LavaCorners_pRNNs/'
    hiddensize: 500
    pRNNtype: thRNN_1win #AutoencoderPred_LN
    action_encoding: SpeedHD #Onehot
    cell: LayerNormRNNCell
    lr: 2e-3
    bptttrunc: 1e8
    weight_decay: 3e-3
    ntimescale: 2
    dropout: 0.15
    noisemean: 0
    noisestd: 0.05
    sparsity: 0.5
    train: False # Train pRNN simultaneously with RL?

logging:
    video_log_freq: 500 # Number of episodes between video logs (0: no videos)
    log_interval: 1 # Number of updates between two logs
    analysis_interval: 25 # Number of updates between analysis events (0: no analysis)
    save_interval: 120 # Number of updates between two saves (0: don't save)
    early_stop: True # Should the training stop when the environment is solved?
    save_params: True # Should the parameters given to the script be saved? (Always...)
    focus: False #'rl.k_int' # Which parameter to include in the group name or False
    project: test # ['Place field width', 'PO-FO-PC-CANN-comparison']
    logdir: "/network/scratch/a/aleksei.efremov/RLstorage"
    load_acmodel: False #'/network/scratch/a/aleksei.efremov/RLstorage/Donut_long_neg05_shortbptt/PO+Rec_seed10_24-01-22-15-46-34' # False or the path to the model
    load_worldmodel: True #True
