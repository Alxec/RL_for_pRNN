defaults:
    - rl/ppo # Choose algorithm subconfig
    - _self_  
    - override hydra/hydra_logging: disabled  
    - override hydra/job_logging: disabled

hydra:  
  output_subdir: null
  sweep:
    dir: .
    subdir: .
  run:  
    dir: .

exp:
    env_name: MiniGrid-LRoom-16x16-v0 # Name of the environment to train on [MiniGrid-FakeLava-5x5-3x4-neg02-v0, MiniGrid-DonutLava-17x17-neg02-v0]
    opt_return: 0.85 # Optimal return for the environment
    # wrapper: 
    #   _target_: minigrid.wrappers.RGBImgPartialObsWrapper_HD # Environment wrapper as python class
    input_type: 'pRNN+PO' # ['Visual_FO', 'Visual_PO', 'PC', 'CANN', 'PC+PO', 'pRNN', 'pRNN+PO','CANN+PO', 'CANN_norecurrence']
    exp_name: defaultName
    seed: 2 # Random seed
    pRNN: True # Wether to use pRNN for spatial input
    PC: False # Wether to use fake place cells for spatial input
    CANN: False # Wether to use CANN for spatial input
    recurrence: 1 # Wether to use RNN for memory (1 = don't use, other number = number of recurrent steps)
    with_obs: True # Wether to include visual input (in case of recurrency the one that would bypass RNN)
    with_HD: True
    rgb: True # Is visual input RGB?
    intrinsic: False # Use intrinsic rewards?
    theta: False
    single_theta: False
    shared_weights: False
    random_action_agent: False # If True, overrides all PPO algorithms and chooses actions randomly. Used as control
    curious_agent: True # Use pRNN loss as reward ?
    random_init_control: False # If True, never update parameters i.e. test randomly initalized agent, usually as a control
    offpolicy_prnn_eval: True # If true, action output is off policy (so if PPO is being done, then actions random. Vice versa if random_action_agent)
    onpolicy_prnn_eval: True # If True, actions chosen at prnn eval (SI, decoding, etc) are chosen on policy. 
    analyze_agent_behav: True # If true, analyze agent policy (plot policy,state distribution,and more, and calculate mutual info between state and action)

hardware:
    use_gpu: False
    which_gpu: 0 # GPU device code, not used now


rl:
    steps: 2.048e7
    discount: 0.98
    lr: 3e-4
    gae_lambda: 0.95 # Lambda coefficient in GAE formula (1 means no gae)
    entropy_coef: 0.0
    value_loss_coef: 1
    max_grad_norm: 0.5
    optim_eps: 1e-8 # Adam and RMSprop optimizer epsilon
    optim_alpha: 0.99 # RMSprop optimizer alpha
    pc_sd: 4
    pc_norm: True
    cann_input_width: 100
    k_int: 1 # Coefficient for intrinsic reward. Only relevant if exp.intrinsic is True
    k_curious: 1 # Coeffecient for curious reward. Only relevant if exp.curious_agent is True
    eval_type: rewards
    value_type: single

predNet:
    path: no_path/thcycRNN_5win_full-theta-s8 #example_nets_donut_long_prev_action/thRNN_5win_prevAct-PrevAct-s8
    foder: ''
    hiddensize: 500
    pRNNtype: thRNN_5win #AutoencoderPred_LN
    action_encoding: SpeedHD #Onehot
    cell: LayerNormRNNCell
    lr: 3e-3
    bptttrunc: 1e8
    weight_decay: 3e-3
    ntimescale: 2
    dropout: 0.15
    noisemean: 0
    noisestd: 0.05
    sparsity: 0.5
    train: True # Train pRNN simultaneously with RL?
    seqdur: 256 # Only relevant if train is on

logging:
    video_log_freq: 500 # Number of episodes between video logs (0: no videos)
    log_interval: 1 # Number of updates between two logs
    analysis_interval: 200 # Number of updates between analysis events (0: no analysis)
    save_interval: 120 # Number of updates between two saves (0: don't save)
    early_stop: False # Should the training stop when the environment is solved?
    save_params: True # Should the parameters given to the script be saved? (Always...)
    focus: False #'rl.k_int' # Which parameter to include in the group name or False
    project: CuriousAgent # ['Place field width', 'PO-FO-PC-CANN-comparison']
    logdir: /network/scratch/h/halawaa/RLstorage
    load_acmodel: False #'/network/scratch/a/aleksei.efremov/RLstorage/Donut_long_neg05_shortbptt/PO+Rec_seed10_24-01-22-15-46-34' # False or the path to the model
    load_worldmodel: False #True
